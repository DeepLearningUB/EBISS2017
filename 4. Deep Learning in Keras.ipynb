{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in `keras`\n",
    "\n",
    "> Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data structure of Keras is a model, a way to organize layers. The main type of model is the ``Sequential model``, a linear stack of layers. \n",
    "\n",
    "```Python\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking layers is as easy as ``.add()``:\n",
    "\n",
    "```Python\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(output_dim=64, input_dim=100))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your model looks good, configure its learning process with ``.compile()``:\n",
    "\n",
    "```Python\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='sgd', metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to, you can further configure your optimizer.\n",
    "\n",
    "```Python\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now iterate on your training data in batches:\n",
    "\n",
    "```Python\n",
    "model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)\n",
    "```\n",
    "\n",
    "Evaluate your performance in one line:\n",
    "```Python\n",
    "loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or generate predictions on new data:\n",
    "\n",
    "```Python\n",
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: MNIST  MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 159s   \n",
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/models.py:851: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.2478 - acc: 0.9238 - val_loss: 0.1089 - val_acc: 0.9671\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1032 - acc: 0.9682 - val_loss: 0.0845 - val_acc: 0.9752\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0770 - acc: 0.9769 - val_loss: 0.0887 - val_acc: 0.9745\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0615 - acc: 0.9816 - val_loss: 0.0907 - val_acc: 0.9749\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.0503 - acc: 0.9851 - val_loss: 0.0808 - val_acc: 0.9793\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0452 - acc: 0.9864 - val_loss: 0.0764 - val_acc: 0.9803\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0387 - acc: 0.9883 - val_loss: 0.0753 - val_acc: 0.9815\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0343 - acc: 0.9898 - val_loss: 0.0801 - val_acc: 0.9821\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0322 - acc: 0.9910 - val_loss: 0.0889 - val_acc: 0.9812\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.0300 - acc: 0.9914 - val_loss: 0.0938 - val_acc: 0.9826\n",
      "\n",
      "\n",
      "Test score: 0.0938096408454\n",
      "Test accuracy: 0.9826\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Trains a simple deep NN on the MNIST dataset.\n",
    "You can get to 98.40% test accuracy after 20 epochs.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test  = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# print model characteristics\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    Y_train,\n",
    "                    batch_size=batch_size, \n",
    "                    nb_epoch=nb_epoch,\n",
    "                    verbose=1, \n",
    "                    validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Name generation with LSTM\n",
    "\n",
    "We are going to train RNN \"character-level\" language models. \n",
    "\n",
    "That is, we’ll give the RNN a huge chunk of text and ask it to model the probability distribution of the next character in the sequence given a sequence of previous characters. This will then allow us to generate new text one character at a time.\n",
    "\n",
    "We will encode each character into a vector using ``1-of-k`` encoding (i.e. all zero except for a single one at the index of the character in the vocabulary), and feed them into the RNN one at a time. \n",
    "\n",
    "At test time, we will feed a character into the RNN and get a distribution over what characters are likely to come next. We sample from this distribution, and feed it right back in to get the next letter. Repeat this process and you’re sampling text!\n",
    "\n",
    "We can also play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process sequences of symbols with RNN we need to represent these symbols by numbers.\n",
    "\n",
    "Let's suppose we have $|V|$ different symbols. The most simple representation is the **one-hot vector**: Represent every symbol as an $\\mathbb{R}^{|V|\\times1}$ vector with all $0$s and one $1$ at the index of that word. Symbol vectors in this type of encoding would appear as the following:\n",
    "\n",
    "$$w^{s_1} = \\left[ \\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right], w^{s_2} = \\left[ \\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{array} \\right], w^{s_3} = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{array} \\right], \\cdots \n",
    "w^{s_{|V|}} = \\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{array} \\right] $$\n",
    "\n",
    "We represent each symbol as a completely independent entity. This symbol representation does not give us directly any notion of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need text to learn from a large dataset of names. Fortunately we don’t need any labels to train a language model, just raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 47527\n",
      "total chars: 30\n",
      "nb sequences: 15836\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import codecs\n",
    "f = codecs.open('data/NombresMujerBarcelona.txt', \"r\", \"utf-8\")\n",
    "#f = codecs.open('data/toponims.txt', \"r\", \"utf-8\")\n",
    "string = f.read()\n",
    "string.encode('utf-8')\n",
    "text = string.lower()\n",
    "\n",
    "# text = text.replace(\"\\n\", \" \")\n",
    "    \n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, \n",
    "               dropout=0.2, \n",
    "               recurrent_dropout=0.2, \n",
    "               input_shape=(maxlen, len(chars))))\n",
    "#model.add(LSTM(64, \n",
    "#               dropout_W=0.2, \n",
    "#               dropout_U=0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. \n",
    "\n",
    "This process is repeated for as long as we want to predict new characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 7s - loss: 3.0414     \n",
      "----- Generating with seed: \"izaskun izza jacinta\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "\n",
      "\n",
      "\n",
      "ia\n",
      "az\n",
      "in\n",
      "e\n",
      "a\n",
      "ua\n",
      "lia\n",
      "ileiad\n",
      "\n",
      "a\n",
      "\n",
      "fiaa\n",
      "\n",
      "anaviara\n",
      "\n",
      "\n",
      "\n",
      "----- diversity: 1.0\n",
      " uxawzañaalaiarneloñrx\n",
      "iarrsclelgoçafe\n",
      "lcaanv iafc\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.8896     \n",
      "----- Generating with seed: \"maria resurreccion m\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "a\n",
      "jiey\n",
      "tahaxwi\n",
      "iaaka\n",
      "lil\n",
      "tariza\n",
      "za\n",
      "ta\n",
      "ia\n",
      "olilela\n",
      "s\n",
      "\n",
      "----- diversity: 1.0\n",
      "ysviqayjszeamky\n",
      "omavws\n",
      "v guyiva asqgefcfjgt\n",
      "e\n",
      "qejl\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.8032     \n",
      "----- Generating with seed: \" josefa rosa josefa \"\n",
      "\n",
      "----- diversity: 0.5\n",
      "uarin\n",
      "xn\n",
      "nan\n",
      "jsaldina\n",
      "jina\n",
      "linna\n",
      "arxizasa\n",
      "via\n",
      "ilil\n",
      "\n",
      "----- diversity: 1.0\n",
      "çnqparzaz\n",
      "oçalwekcfa\n",
      "rmu\n",
      "jana lrnsamarhlqssaxng\n",
      "ca\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 7s - loss: 2.7497     \n",
      "----- Generating with seed: \"la anabella anacleta\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "\n",
      "daria\n",
      "anaciaqisa\n",
      "hariaora\n",
      "luraa\n",
      "jangaçina\n",
      "ñalia\n",
      "l\n",
      "\n",
      "----- diversity: 1.0\n",
      "ararqazaxianrwa\n",
      "raoxigtala\n",
      "f\n",
      "mzcrijmar riu\n",
      "aciqmao\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 9s - loss: 2.7063     \n",
      "----- Generating with seed: \"izabeth viviane volh\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "jina\n",
      "kyrlava fria\n",
      "zaril\n",
      "nka\n",
      "ura\n",
      "lansna\n",
      "ulel ela\n",
      "be\n",
      "\n",
      "----- diversity: 1.0\n",
      "luojjha\n",
      "crorbe\n",
      "qla gkpen\n",
      "ka\n",
      "ese\n",
      "leqa lbadw mfrizce\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 9s - loss: 2.6326     \n",
      "----- Generating with seed: \"viviana viviana andr\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "ia\n",
      "auria\n",
      "aha\n",
      "anana\n",
      "dincia\n",
      "blatia\n",
      "ñarina\n",
      "varina\n",
      "nag\n",
      "\n",
      "----- diversity: 1.0\n",
      "ril jerrnvka\n",
      "vinjnla\n",
      "çjqi\n",
      "kva epirisga\n",
      "cerseg\n",
      "\n",
      "pet\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.6107     \n",
      "----- Generating with seed: \"s montserrat dolores\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "a alela labela\n",
      "ranera\n",
      "naria\n",
      "tannene\n",
      "sasosa\n",
      "lasia a\n",
      "\n",
      "----- diversity: 1.0\n",
      "nha\n",
      "sejmara dlwhe ylzsswinanana\n",
      "oka zarewa\n",
      "elejvea\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.5867     \n",
      "----- Generating with seed: \"ra carolina sandra c\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "eria  anena\n",
      "tinta\n",
      "zania wanvia elina\n",
      "anosa olen te\n",
      "\n",
      "----- diversity: 1.0\n",
      "nn\n",
      "ana ujis ñiztziow\n",
      "nakfaiam\n",
      "lianona manweuz naq \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.5652     \n",
      "----- Generating with seed: \"ian mariana mariana \"\n",
      "\n",
      "----- diversity: 0.5\n",
      "elina\n",
      "elevia\n",
      "jjenwa\n",
      "l driba\n",
      "ala elia\n",
      "sali\n",
      "çanse\n",
      "ci\n",
      "\n",
      "----- diversity: 1.0\n",
      "ynd\n",
      "ala\n",
      "dretsa\n",
      "hdriq naçfaa isqpuvg\n",
      "liunjter\n",
      "ulery\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.5621     \n",
      "----- Generating with seed: \"lda casimira cassand\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "a\n",
      "yenle\n",
      "qiixan\n",
      "ertena\n",
      "salia usajela\n",
      "naria ança\n",
      "fan\n",
      "\n",
      "----- diversity: 1.0\n",
      "\n",
      "orh eraria  lify\n",
      "jllina\n",
      "faelxçerajsvgyraeray\n",
      "ñiec\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.5189     \n",
      "----- Generating with seed: \"yina ying yingying y\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "nana\n",
      "diandra\n",
      "safia elisa\n",
      "ana wania\n",
      "zaelia\n",
      "anita al\n",
      "\n",
      "----- diversity: 1.0\n",
      "eja\n",
      "nsviinqayaç añeke\n",
      "yrsenilia\n",
      "br\n",
      "prctjen anxelia\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.5019     \n",
      "----- Generating with seed: \"eraldine gerarda ger\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "eneu\n",
      "parenina\n",
      "aleña\n",
      "ane\n",
      "lara\n",
      "ñarina\n",
      "zaria\n",
      "ala kaur\n",
      "\n",
      "----- diversity: 1.0\n",
      "ea\n",
      "ur filly\n",
      "sangreha alipaqserana\n",
      "vçmanira çmqdjan\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.4912     \n",
      "----- Generating with seed: \"olina cynthia elizab\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "el\n",
      "ana iana\n",
      "kaana elana\n",
      "uxisana\n",
      "naria rana\n",
      "ana ana\n",
      "\n",
      "----- diversity: 1.0\n",
      "pvitewarta\n",
      "xiaxhtaubey\n",
      "maria inwzzssae\n",
      "onxjkah\n",
      "sxn\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.4778     \n",
      "----- Generating with seed: \"a esther sara isabel\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "\n",
      "anfeladra\n",
      "andra ñneelia\n",
      "antzaxanwen\n",
      "ana uela\n",
      "sare\n",
      "\n",
      "----- diversity: 1.0\n",
      "pinalla\n",
      "uñagelvinnryosipaelpçeh\n",
      "paaldgevipiha\n",
      "fafs\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 8s - loss: 2.4490     \n",
      "----- Generating with seed: \" benita benjamina be\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "neça ççenia\n",
      "rtisa rswin\n",
      "ana raocia\n",
      "ana yanyra\n",
      "ana \n",
      "\n",
      "----- diversity: 1.0\n",
      "valiñwana\n",
      "açfi ilofia\n",
      "ckabitzcxfamhxage sllipm\n",
      "dñk\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 7s - loss: 2.4319     \n",
      "----- Generating with seed: \"lotilde colette colo\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "rina\n",
      "lika\n",
      "gla\n",
      "liana\n",
      "anica\n",
      "olisa\n",
      "ançia esesisa\n",
      "ana \n",
      "\n",
      "----- diversity: 1.0\n",
      "kases\n",
      "bmalia harfonta\n",
      "svala weq\n",
      "narmeo\n",
      "elnefñp\n",
      "rxm\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Epoch 1/1\n",
      "15836/15836 [==============================] - 9s - loss: 2.4165     \n",
      "----- Generating with seed: \"sca juana isabel jua\"\n",
      "\n",
      "----- diversity: 0.5\n",
      "na elisa\n",
      "sacile\n",
      "ina\n",
      "ñaj elina\n",
      "qiana\n",
      "esezya\n",
      "ana eli\n",
      "\n",
      "----- diversity: 1.0\n",
      "nila\n",
      "atz elemne\n",
      "parles gr elaz\n",
      "farnekça narqeq\n",
      "mar\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Epoch 1/1\n",
      " 3328/15836 [=====>........................] - ETA: 7s - loss: 2.3473"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e4fae71cd66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mstart_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=256, epochs=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence.replace(\"\\n\", \" \") + '\"')\n",
    "        \n",
    "    for diversity in [0.5, 1.0]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "        for i in range(50):\n",
    "            \n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demo\n",
    "\n",
    "https://transcranial.github.io/keras-js/#/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
